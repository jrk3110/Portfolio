---
title: "Marketing Sales Data Analysis"
author: "Jordan King"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## About the Data Set

This data set is sourced from Kaggle at the following URL: https://www.kaggle.com/datasets/yakhyojon/marketing-promotion/data
The data do not belong to me and all rights are reserved by the original owner.

Each row corresponds to an independent marketing promotion where their business uses TV, social media, radio, and influencer promotions to increase sales.
The features in the data are:

- TV promotional budget (in "Low," "Medium," and "High" categories)
- Social media promotional budget (in millions of dollars)
- Radio promotional budget (in millions of dollars)
- Sales (in millions of dollars)
- Influencer size (in "Mega," "Macro," "Micro," and "Nano" categories)

## Load Necessary Packages

``` {r echo=TRUE, message = F, warning = F}
# family of packages with many useful functions
library(tidyverse)
# package containing function "getURL", allowing data to be read from GitHub
library(RCurl)
# package containing function to compute all pairwise comparisons
library(emmeans)
# package containing function "regsubsets" for computing AIC
library(leaps)
```

## Read in Data

```{r echo = TRUE, message = F, warning = F}
url <- getURL("https://raw.githubusercontent.com/jrk3110/Portfolio/refs/heads/main/marketing_sales_data.csv")
marketing_data <- read_csv(url, sep = ",")
```

## Clean the Data

First, let's take a look at the data.

```{r echo = TRUE}
# View first few rows of data
head(marketing_data)

# View a summary of the data
summary(marketing_data)
```

Now, let's clean the data.

``` {r echo = TRUE}
marketing_data <- marketing_data %>% 
                    # Change data type to "factor"
                    mutate(TV = as.factor(TV)) %>% 
                    mutate(Influencer = as.factor(Influencer)) %>% 
                    # Change variable names to lower case
                    rename(tv = TV) %>% 
                    rename(influencer = Influencer) %>% 
                    rename(radio = Radio) %>% 
                    rename(social_media = "Social Media") %>% 
                    rename(sales = Sales) %>% 
                    # Reorder columns so response is first
                    select(sales, tv, radio, social_media, influencer)

# View new data summary
summary(marketing_data)
```

Looking at our new data summary, there don't seem to be any obvious issues with the data. Let's take a closer look just to be sure.

``` {r echo = TRUE}
# Check for missing values
marketing_data %>% 
  sapply(anyNA)
```

There are no missing values in this data set. We are now ready to start exploring the data.

## Explore the Data

Let's create some basic plots to get a sense of the shape of the data and identify any potential patterns or areas of intrigue.

### Sales vs. Each Variable

```{r echo = TRUE}
marketing_data %>% 
  ggplot(aes(x = tv, y = sales, fill = tv)) +
    geom_violin(trim = F) +
    geom_boxplot(width = .3) +
    geom_jitter(width = .05, size = 1, alpha = .3) +
    theme_bw() +
    scale_x_discrete(limits = c("Low", "Medium", "High")) +
    labs(title = "Sales by TV Budget",
         x = "TV Budget",
         y = "Sales, in millions of US Dollars")
```

There appears to be significant differences in `sales` between each level of `tv`. We will be sure to run a statistical test on this later.

```{r echo = TRUE}
marketing_data %>% 
  ggplot(aes(x = radio, y = sales)) +
    geom_point() +
    theme_bw() +
    labs(title = "Sales by Radio Budget",
         x = "Radio Budget, in millions of US Dollars",
         y = "Sales, in millions of US Dollars")

# Get correlation coefficient
cor(marketing_data$radio, marketing_data$sales)
```

There is an obvious positive linear relationship between `radio` and `sales`. In fact, the correlation coefficient between the two variables is `0.858`, which is very high.

```{r echo = TRUE}
marketing_data %>% 
  ggplot(aes(x = influencer, y = sales, fill = influencer)) +
    geom_violin(trim = F) +
    geom_boxplot(width = .3) +
    geom_jitter(width = .05, size = 1, alpha = .3) +
    theme_bw() +
    scale_x_discrete(limits = c("Nano", "Micro", "Macro", "Mega")) +
    labs(title = "Sales by Influencer Size",
         x = "Influencer Size",
         y = "Sales, in millions of US Dollars")
```

`influencer` does not seem to be an important variable in this analysis, as `sales` do not seem to differ significantly among `influencer` levels.

```{r echo = TRUE}
marketing_data %>% 
  ggplot(aes(x = social_media, y = sales)) +
    geom_point() +
    theme_bw() +
    labs(title = "Sales by Social Media Budget",
         x = "Social Media Budget, in millions of US Dollars",
         y = "Sales, in millions of US Dollars")

cor(marketing_data$social_media, marketing_data$sales)
```

There seems to be a positive linear relationship between `social_media` and `sales`; however, this relationship is not quite as strong as the relationship between `radio` and `sales`, as the correlation coefficient is `0.542` (although that is still a moderately strong correlation).

## Statistical Tests

Now that we have explored our data and identified some areas of intrigue, let's run some formal statistical tests so we can make evidence-based inferences about the data.

### Find the Best Linear Model

```{r echo = TRUE}
# Use AIC to find best model
n = dim(marketing_data)[1]
reg1 = regsubsets(sales ~ ., data = marketing_data)
rs = summary(reg1)

# Compute and plot AIC for each size linear model
aic = 2*(2:8) + n*log(rs$rss/n)
plot(aic ~ I(1:7))

# Select best choice
df.aic <- data.frame(x = 1:7, y = aic)
aic.min <- df.aic$x[which.min(df.aic$y)]
paste0("Choice: ", aic.min, " predictors")

# Gives table of predictors to include in the best linear model with 3 predictors
rs$which[3, ]
```

According to the AIC, the best model (i.e., the model with the lowest AIC) is given by the 3 predictors `tv_low`, `tv_medium`, and `radio`. Of course, `intercept` includes the third level of the `tv` variable, namely `tv_high`. So, our final linear model should include the predictors `tv` and `radio` against the response `sales`.

Now, let's see if there is a statistically significant interaction effect between `tv` and `radio`.

``` {r echo = TRUE}
# Check if interaction is significant
lm_final <- lm(sales ~ tv + radio, data = marketing_data)
lm_final_int <- lm(sales ~ tv * radio, data = marketing_data)

anova(lm_final, lm_final_int)
```

Because the p-value of this ANOVA is $>0.05$, we can conclude that the model *without* the interaction is sufficient.

Let's look at the summary for our final linear model.

``` {r echo = TRUE}
summary(lm_final)
```

The adjusted $R^2$ for this linear model is `0.9035`, which indicates that this model explains approximately $90\%$ of the data. Very good!

Let's conduct an additional test to make it clear whether all levels of `tv` are statistically significantly different from each other.

```{r echo = TRUE}
pairs(lsmeans(lm_final, "tv"), adjust = "bonferroni")
```

We can see that each level of `tv` is, in fact, statistically significantly different from each other level. Let's revisit the plot that shows this relationship:

```{r echo = TRUE}
marketing_data %>% 
  ggplot(aes(x = tv, y = sales, fill = tv)) +
    geom_violin(trim = F) +
    geom_boxplot(width = .3) +
    geom_jitter(width = .05, size = 1, alpha = .3) +
    theme_bw() +
    scale_x_discrete(limits = c("Low", "Medium", "High")) +
    labs(title = "Sales by TV Budget",
         x = "TV Budget",
         y = "Sales, in millions of US Dollars")
```

Let's also revisit the plot that shows the relationship between `radio` and `sales`, this time with a linear model of `sales` ~ `radio` included in the plot.

```{r echo = TRUE, warning = FALSE, message = FALSE}
marketing_data %>% 
  ggplot(aes(x = radio, y = sales)) +
    geom_point(alpha = 0.7) +
    geom_smooth(method = lm, se = F, color = "blue") +
    theme_bw() +
    labs(title = "Sales by Radio Budget",
         x = "Radio Budget, in millions of US Dollars",
         y = "Sales, in millions of US Dollars")
```

Now, let's visualize our final linear model with both the `radio` and `tv` predictors against the `sales` response.

```{r echo = TRUE, warning = FALSE, message = FALSE}
marketing_data %>% 
  ggplot(aes(x = radio, y = sales, color = tv)) +
    geom_point() +
    geom_smooth(method = lm, se = F) +
    theme_bw() +
    labs(title = "Sales by Radio and TV Budgets",
         x = "Radio Budget, in millions of US Dollars",
         y = "Sales, in millions of US Dollars",
         color = "TV Budget")
```

### Check Assumptions

``` {r echo = TRUE}
# Fitted vs Observed plot
marketing_data %>% 
  ggplot(aes(x = lm_final$fitted, y = sales)) +
    geom_point() +
    theme_bw() +
    labs(title = "Fitted vs. Observed Values",
         x = "Fitted Values",
         y = "Observed Values")

# Fitted vs Residual plot
marketing_data %>% 
  ggplot(aes(x = lm_final$fitted, y = lm_final$resid)) +
    geom_point() +
    theme_bw() +
    labs(title = "Fitted Values vs. Residuals",
         x = "Fitted Values",
         y = "Residuals")

# QQ plot
qqnorm(lm_final$resid)
qqline(lm_final$resid, col = "blue")

# Shapiro-Wilk test
shapiro.test(lm_final$resid)
```

**Insights:**

- The Linearity assumption is met. The Fitted vs. Observed values plot shows gentle scatter around the line $y=\hat y$ with no noticeable curvature. The Fitted vs. Residual plot shows scatter around the line $y=0$. While the points are clustered in three groups, that is because the three-level `tv` factor has a prominent affect on the data and does not invalidate the linearity assumption.
- The Normality assumption may be met. The Shapiro-Wilk test gives a p-value $<0.05$, which provides evidence that the data are *not* normally distributed. However, the Q-Q plot above provides evidence that the data are generally normally distributed, with some variation at each end of the plot. Given the facts that the Shapiro-Wilk test is notoriously sensitive, the Q-Q plot showed nothing overly concerning, and close normality is typically an accepted qualification of the normality assumption, I assert that in this case, we can carry on as if the normality assumption is met.
- The Independence assumption is met because each row of the data set is an independent observation, as noted in the data description.
- The Homoskedasticity (constant variance) assumption is met. There is no "funnel" shape to the points in the Fitted vs. Residual plot. Although the points are clustered into three groups due to the `tv` factor, each cluster appears to have a similar shape, thus implying constant variance among the groups.
